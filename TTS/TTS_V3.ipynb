{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care\n",
    "!pip install -q espnet==0.10.6 pyopenjtalk==0.2 pypinyin==0.44.0 parallel_wavegan==0.5.4 gdown==4.4.0 espnet_model_zoo pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title English multi-speaker pretrained model { run: \"auto\" }\n",
    "lang = 'English'\n",
    "tag2 = 'kan-bayashi/libritts_xvector_vits' #@param /[\"kan-bayashi/vctk_gst_tacotron2\", \"kan-bayashi/vctk_gst_transformer\", \"kan-bayashi/vctk_xvector_tacotron2\", \"kan-bayashi/vctk_xvector_transformer\", \"kan-bayashi/vctk_xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_gst+xvector_tacotron2\", \"kan-bayashi/vctk_gst+xvector_transformer\", \"kan-bayashi/vctk_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_multi_spk_vits\", \"kan-bayashi/vctk_full_band_multi_spk_vits\", \"kan-bayashi/libritts_xvector_transformer\", \"kan-bayashi/libritts_xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_gst+xvector_transformer\", \"kan-bayashi/libritts_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_xvector_vits\"] {type:\"string\"}\n",
    "vocoder_tag2 = \"none\" #@param [\"none\", \"parallel_wavegan/vctk_parallel_wavegan.v1.long\", \"parallel_wavegan/vctk_multi_band_melgan.v2\", \"parallel_wavegan/vctk_style_melgan.v1\", \"parallel_wavegan/vctk_hifigan.v1\", \"parallel_wavegan/libritts_parallel_wavegan.v1.long\", \"parallel_wavegan/libritts_multi_band_melgan.v2\", \"parallel_wavegan/libritts_hifigan.v1\", \"parallel_wavegan/libritts_style_melgan.v1\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "from espnet2.utils.types import str_or_none\n",
    "import torch\n",
    "from IPython.display import display, Audio\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import kaldiio\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import soundfile as sf\n",
    "from espnet_model_zoo.downloader import ModelDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tag2 = 'kan-bayashi/libritts_xvector_vits'\n",
    "vocoder_tag2 = \"none\"\n",
    "\n",
    "\n",
    "text2speech = Text2Speech.from_pretrained(\n",
    "    model_tag=str_or_none(tag2),\n",
    "    vocoder_tag=str_or_none(vocoder_tag2),\n",
    "    #device=\"cuda\",\n",
    "    speed_control_alpha=0.95,\n",
    "    noise_scale=0.333,\n",
    "    noise_scale_dur=0.3,\n",
    ")\n",
    "\n",
    "d = ModelDownloader()\n",
    "model_dir = os.path.dirname(d.download_and_unpack(tag2)[\"train_config\"])\n",
    "\n",
    "# X-vector selection\n",
    "spembs = None\n",
    "if text2speech.use_spembs:\n",
    "    xvector_ark = [p for p in glob.glob(f\"{model_dir}/../../dump/**/spk_xvector.ark\", recursive=True) if \"tr\" in p][0]\n",
    "    xvectors = {k: v for k, v in kaldiio.load_ark(xvector_ark)}\n",
    "    spks = list(xvectors.keys())\n",
    "\n",
    "    # randomly select speaker\n",
    "    random_spk_idx = np.random.randint(0, len(spks))\n",
    "    spk = spks[random_spk_idx]\n",
    "    spembs = xvectors[spk]#xvectors[spks[2]]\n",
    "    print(f\"selected spk: {spk}\")\n",
    "\n",
    "# Speaker ID selection\n",
    "sids = None #'p300'\n",
    "if text2speech.use_sids:\n",
    "    spk2sid = glob.glob(f\"{model_dir}/../../dump/**/spk2sid\", recursive=True)[0]\n",
    "    with open(spk2sid) as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    sid2spk = {int(line.split()[1]): line.split()[0] for line in lines}\n",
    "    \n",
    "    # randomly select speaker\n",
    "    sids = np.array(np.random.randint(1, len(sid2spk)))\n",
    "    spk = sid2spk[int(sids)]\n",
    "    print(f\"selected spk1: {spk}\")\n",
    "\n",
    "# Reference speech selection for GST\n",
    "speech = None\n",
    "if text2speech.use_speech:\n",
    "    # you can change here to load your own reference speech\n",
    "    # e.g.\n",
    "    # import soundfile as sf\n",
    "    # speech, fs = sf.read(\"/path/to/reference.wav\")\n",
    "    # speech = torch.from_numpy(speech).float()\n",
    "    speech = torch.randn(50000,) * 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts=\"\"\"This section helps you categorize your problem in order to realize the role of AI/ML in finding a solution to your business problem. \n",
    "Which bucket does your problem fall under? \n",
    "A First Principles problem can be realized when an existing approach to address an issue can be found.  In short, there exists a known methodology to solve the problem. Methodical disassembling of a computer to repair its defective parts may be considered as a first principles problem. \n",
    "The emotional, strategic, legal, and ethical bucket of problems are typically found in your business. Picking an ideal applicant for employment, addressing a customer churn problem, avoiding discrimination and bias, conducting business within your personal & professional morals and values some business scenarios that fall under this bucket.\n",
    "A logical or a rational business problem is that which you know has reason but not know of an approach towards its solution.  \n",
    "Ensure to categorize your business problem into one of these buckets, divide the problem further, if necessary. This helps to optimize your AI/ML approach to its best potential.  \n",
    "If your organization is in the beginning phases of adapting AI, we recommend for you to pick problems that fall under the third bucket.\n",
    "Why? \n",
    "For solving the first bucket, AI/ML mostly aid in minimizing the number of choices or parameters required by a human expert to execute the known optimal approach. The second bucket requires multiple human experts and AI/ML systems to find an optimal solution. However, AI/ML systems are just aids for the experts.\n",
    "The third bucket is where an AI/ML system is your front-end problem-solving contributor. Predicting a stock price can be a good example for this bucket. Picking third-bucket problems will thus help you immediately realize the value of AI/ML systems.  \n",
    "If there is insufficient data for you to incorporate the systems, focus on collecting the data.  \n",
    "For any third-bucket problem, it is efficient to quickly incorporate AI/ML systems for problem-solving.\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'2428_83705'**,** '251_137823'**,** '251_118436'**,** '3752_4944'**,** '422_122949'\n",
    "\n",
    "tag2 = 'kan-bayashi/libritts_xvector_vits'\n",
    "\n",
    "vocoder_tag2 = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=['251_137823','251_118436','422_122949']\n",
    "#1:251_137823,2:251_118436,3:422_122949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "for i in y:\n",
    "  with torch.no_grad():\n",
    "    start = time.time()\n",
    "    wav = text2speech(tts, speech=speech, spembs=xvectors[i], sids=sids)[\"wav\"]\n",
    "    rtf = (time.time() - start) / (len(wav) / text2speech.fs)\n",
    "    print(f\"RTF = {rtf:5f}\")\n",
    "    print(i)\n",
    "    display(Audio(wav.view(-1).cpu().numpy(),rate=text2speech.fs))\n",
    "    sf.write(\"speech_\"+str(j)+\".wav\", wav.to('cpu').numpy(), 22050)\n",
    "    sound = pydub.AudioSegment.from_wav(\"/content/speech_\"+str(j)+\".wav\")\n",
    "    sound.export(\"voice_\"+str(j)+\".mp3\", format=\"mp3\")\n",
    "    j=j+1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
